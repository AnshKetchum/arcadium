parameters:
  experiment_name: "nano-gpt-speedrun"
  epochs: 10000
  val_steps: 10
  checkpoint_frequency: 100  # keeping as requested
  batch_size: 32              # increased from 8
  num_workers: 4              # keeping as requested
  sequence_length: 1024       # increased from 32
  lr: 0.001                  # decreased from 0.001
  optimizer:
    name: "muon+adam"
    lr: 0.001                  # decreased from 0.001
  training_sequence_length:
    start: 1024               # fixed length, no curriculum
    end: 1024                 # fixed length, no curriculum
    steps: 1                  # no curriculum steps
  validation_sequence_length: 1024  # increased from 256
  training_data_config: "configs/data/fineweb-gpt-2.yaml"
  validation_data_config: "configs/data/fineweb-gpt-2.yaml"
  warmup_steps: 2000          # explicit warmup
  min_lr_ratio: 0.1           # GPT-2 style decay