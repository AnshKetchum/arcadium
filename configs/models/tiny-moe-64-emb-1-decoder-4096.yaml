parameters:
  name: "moe-basic-1m-64-emb"
  type: "moe"
  configuration:
    model:
      decoder_layers: 1 
    decoder: 
      input_dimension: 64
      output_dimension: 64
      hidden_dimension: 64
      experts: 8
      norm_eps: 1e-5
      attention:
        type: "multi-head"
        num_query_heads: 8
        num_kv_heads: 8
        embedding_dimension: 64
        head_dimension: 32

language_model:
  vocab_size: 4096