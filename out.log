Training Config: {'experiment_name': 'moe-language-modeling-tiny', 'epochs': 1000, 'val_steps': 10, 'checkpoint_frequency': 100, 'batch_size': 8, 'num_workers': 2, 'sequence_length': 32, 'lr': 0.001, 'training_data_config': 'configs/data/medium-corpus.yaml', 'validation_data_config': 'configs/data/medium-corpus.yaml'}
Default initialization
Configuring ...  moe-basic-1m-64-emb-1L
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
LanguageModel                                      [1, 32, 8192]             --
â”œâ”€Embedding: 1-1                                   [1, 32, 64]               524,288
â”œâ”€MoETransformer: 1-2                              [1, 32, 64]               --
â”‚    â””â”€ModuleList: 2-1                             --                        --
â”‚    â”‚    â””â”€MoEDecoder: 3-1                        [1, 32, 64]               116,872
â”‚    â”‚    â””â”€MoEDecoder: 3-2                        [1, 32, 64]               116,872
â”‚    â”‚    â””â”€MoEDecoder: 3-3                        [1, 32, 64]               116,872
â”‚    â”‚    â””â”€MoEDecoder: 3-4                        [1, 32, 64]               116,872
â”‚    â”‚    â””â”€MoEDecoder: 3-5                        [1, 32, 64]               116,872
â”‚    â”‚    â””â”€MoEDecoder: 3-6                        [1, 32, 64]               116,872
â”‚    â”‚    â””â”€MoEDecoder: 3-7                        [1, 32, 64]               116,872
â”‚    â”‚    â””â”€MoEDecoder: 3-8                        [1, 32, 64]               116,872
â”‚    â””â”€RMSNorm: 2-2                                [1, 32, 64]               64
â”‚    â””â”€Linear: 2-3                                 [1, 32, 64]               4,096
â”œâ”€Linear: 1-3                                      [1, 32, 8192]             524,288
====================================================================================================
Total params: 1,987,712
Trainable params: 1,987,712
Non-trainable params: 0
Total mult-adds (M): 7.71
====================================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 3.74
Params size (MB): 5.55
Estimated Total Size (MB): 9.29
====================================================================================================
Loaded 4 files for random sampling with sequence length 32
Loaded 4 files for random sampling with sequence length 32
Successfully saved 4432 tokens.
[iter 0] Loss 9.0825, iters/sec 3.37, Tokens 256, Params 7.6MB, Opt 15.2MB, Batch 0.0MB, Acts 23.8MB, VRAM 56.7MB (peak 64.3MB), Fwd 79.2/79.2MB, Bwd 41.5/95.2MB, Opt step 56.7/64.3MB
[iter 0] Saved checkpoint to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-iter0.pt
[iter 0] Saved activation VRAM JSON to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-activations-iter0.json
[iter 0] Saved training metrics JSON to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-metrics-iter0.json
[iter 0] Saved attention visualizations to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/attn_maps_iter0
[iter 0] Validation avg loss: 9.0424
[iter 100] Loss 6.7360, iters/sec 6.27, Tokens 25856, Params 7.6MB, Opt 15.2MB, Batch 0.0MB, Acts 23.8MB, VRAM 56.7MB (peak 64.3MB), Fwd 101.9/109.9MB, Bwd 56.7/117.9MB, Opt step 56.7/64.3MB
[iter 100] Saved checkpoint to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-iter100.pt
[iter 100] Saved activation VRAM JSON to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-activations-iter100.json
[iter 100] Saved training metrics JSON to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-metrics-iter100.json
[iter 100] Saved attention visualizations to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/attn_maps_iter100
[iter 100] Validation avg loss: 6.7696
[iter 200] Loss 5.3833, iters/sec 5.38, Tokens 51456, Params 7.6MB, Opt 15.2MB, Batch 0.0MB, Acts 23.8MB, VRAM 56.7MB (peak 64.3MB), Fwd 102.5/110.5MB, Bwd 56.7/118.5MB, Opt step 56.7/64.3MB
[iter 200] Saved checkpoint to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-iter200.pt
[iter 200] Saved activation VRAM JSON to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-activations-iter200.json
[iter 200] Saved training metrics JSON to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-metrics-iter200.json
[iter 200] Saved attention visualizations to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/attn_maps_iter200
[iter 200] Validation avg loss: 5.2684
[iter 300] Loss 4.2053, iters/sec 5.18, Tokens 77056, Params 7.6MB, Opt 15.2MB, Batch 0.0MB, Acts 23.8MB, VRAM 56.7MB (peak 64.3MB), Fwd 102.5/110.5MB, Bwd 56.7/118.5MB, Opt step 56.7/64.3MB
[iter 300] Saved checkpoint to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-iter300.pt
[iter 300] Saved activation VRAM JSON to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-activations-iter300.json
[iter 300] Saved training metrics JSON to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-metrics-iter300.json
[iter 300] Saved attention visualizations to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/attn_maps_iter300
[iter 300] Validation avg loss: 3.9897
[iter 400] Loss 3.8409, iters/sec 4.61, Tokens 102656, Params 7.6MB, Opt 15.2MB, Batch 0.0MB, Acts 23.8MB, VRAM 56.7MB (peak 64.3MB), Fwd 102.5/110.5MB, Bwd 56.7/118.5MB, Opt step 56.7/64.3MB
[iter 400] Saved checkpoint to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-iter400.pt
[iter 400] Saved activation VRAM JSON to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-activations-iter400.json
[iter 400] Saved training metrics JSON to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-metrics-iter400.json
[iter 400] Saved attention visualizations to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/attn_maps_iter400
[iter 400] Validation avg loss: 3.1395
[iter 500] Loss 2.7149, iters/sec 5.58, Tokens 128256, Params 7.6MB, Opt 15.2MB, Batch 0.0MB, Acts 23.8MB, VRAM 56.7MB (peak 64.3MB), Fwd 102.5/110.5MB, Bwd 56.7/118.5MB, Opt step 56.7/64.3MB
[iter 500] Saved checkpoint to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-iter500.pt
[iter 500] Saved activation VRAM JSON to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-activations-iter500.json
[iter 500] Saved training metrics JSON to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-metrics-iter500.json
[iter 500] Saved attention visualizations to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/attn_maps_iter500
[iter 500] Validation avg loss: 2.7295
[iter 600] Loss 2.1834, iters/sec 3.87, Tokens 153856, Params 7.6MB, Opt 15.2MB, Batch 0.0MB, Acts 23.8MB, VRAM 56.7MB (peak 64.3MB), Fwd 102.5/110.5MB, Bwd 56.7/118.5MB, Opt step 56.7/64.3MB
[iter 600] Saved checkpoint to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-iter600.pt
[iter 600] Saved activation VRAM JSON to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-activations-iter600.json
[iter 600] Saved training metrics JSON to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-metrics-iter600.json
[iter 600] Saved attention visualizations to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/attn_maps_iter600
[iter 600] Validation avg loss: 2.1885
[iter 700] Loss 1.9937, iters/sec 7.06, Tokens 179456, Params 7.6MB, Opt 15.2MB, Batch 0.0MB, Acts 23.8MB, VRAM 56.7MB (peak 64.3MB), Fwd 102.5/110.5MB, Bwd 56.7/118.5MB, Opt step 56.7/64.3MB
[iter 700] Saved checkpoint to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-iter700.pt
[iter 700] Saved activation VRAM JSON to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-activations-iter700.json
[iter 700] Saved training metrics JSON to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-metrics-iter700.json
[iter 700] Saved attention visualizations to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/attn_maps_iter700
[iter 700] Validation avg loss: 1.7210
[iter 800] Loss 1.4374, iters/sec 5.05, Tokens 205056, Params 7.6MB, Opt 15.2MB, Batch 0.0MB, Acts 23.8MB, VRAM 56.7MB (peak 64.3MB), Fwd 102.5/110.5MB, Bwd 56.7/118.5MB, Opt step 56.7/64.3MB
[iter 800] Saved checkpoint to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-iter800.pt
[iter 800] Saved activation VRAM JSON to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-activations-iter800.json
[iter 800] Saved training metrics JSON to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-metrics-iter800.json
[iter 800] Saved attention visualizations to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/attn_maps_iter800
[iter 800] Validation avg loss: 1.4582
[iter 900] Loss 1.6694, iters/sec 4.54, Tokens 230656, Params 7.6MB, Opt 15.2MB, Batch 0.0MB, Acts 23.8MB, VRAM 56.7MB (peak 64.3MB), Fwd 102.5/110.5MB, Bwd 56.7/118.5MB, Opt step 56.7/64.3MB
[iter 900] Saved checkpoint to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-iter900.pt
[iter 900] Saved activation VRAM JSON to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-activations-iter900.json
[iter 900] Saved training metrics JSON to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-metrics-iter900.json
[iter 900] Saved attention visualizations to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/attn_maps_iter900
[iter 900] Validation avg loss: 1.2219
[iter 999] Loss 1.2090, iters/sec 0.58, Tokens 256000, Params 7.6MB, Opt 15.2MB, Batch 0.0MB, Acts 23.8MB, VRAM 56.7MB (peak 64.3MB), Fwd 102.5/110.5MB, Bwd 56.7/118.5MB, Opt step 56.7/64.3MB
[iter 999] Saved checkpoint to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-iter999.pt
[iter 999] Saved activation VRAM JSON to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-activations-iter999.json
[iter 999] Saved training metrics JSON to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/moe-basic-1m-64-emb-1L-metrics-iter999.json
[iter 999] Saved attention visualizations to checkpoints/moe-language-modeling-tiny-moe-basic-1m-64-emb-1L-2025-09-10-13:00:49/attn_maps_iter999
[iter 999] Validation avg loss: 1.1157
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33msmooth-terrain-61[0m at: [34mhttps://wandb.ai/anshc/moe-language-modeling-tiny/runs/4pkk3wnx[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250910_130048-4pkk3wnx/logs[0m
